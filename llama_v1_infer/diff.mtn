diff --git a/example.py b/example.py
index fba9a54..b1c71e8 100755
--- a/example.py
+++ b/example.py
@@ -22,7 +22,7 @@ def setup_model_parallel() -> Tuple[int, int]:
 
     torch.distributed.init_process_group("nccl")
     initialize_model_parallel(world_size)
-    torch.cuda.set_device(local_rank)
+    # torch.cpu.set_device(local_rank)
 
     # seed must be the same in all processes
     torch.manual_seed(1)
@@ -44,7 +44,7 @@ def load(
     ), f"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}"
     ckpt_path = checkpoints[local_rank]
     print("Loading")
-    checkpoint = torch.load(ckpt_path, map_location="cpu")
+    # checkpoint = torch.load(ckpt_path, map_location="cpu")
     with open(Path(ckpt_dir) / "params.json", "r") as f:
         params = json.loads(f.read())
 
@@ -53,10 +53,10 @@ def load(
     )
     tokenizer = Tokenizer(model_path=tokenizer_path)
     model_args.vocab_size = tokenizer.n_words
-    torch.set_default_tensor_type(torch.cuda.HalfTensor)
+    # torch.set_default_tensor_type(torch.HalfTensor)
     model = Transformer(model_args)
     torch.set_default_tensor_type(torch.FloatTensor)
-    model.load_state_dict(checkpoint, strict=False)
+    # model.load_state_dict(checkpoint, strict=False)
 
     generator = LLaMA(model, tokenizer)
     print(f"Loaded in {time.time() - start_time:.2f} seconds")
@@ -79,40 +79,54 @@ def main(
         ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size
     )
 
+    print(generator)
+
     prompts = [
-        # For these prompts, the expected answer is the natural continuation of the prompt
+        # # For these prompts, the expected answer is the natural continuation of the prompt
         "I believe the meaning of life is",
         "Simply put, the theory of relativity states that ",
         "Building a website can be done in 10 simple steps:\n",
-        # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api
-        """Tweet: "I hate it when my phone battery dies."
-Sentiment: Negative
-###
-Tweet: "My day has been 👍"
-Sentiment: Positive
-###
-Tweet: "This is the link to the article"
-Sentiment: Neutral
-###
-Tweet: "This new music video was incredibile"
-Sentiment:""",
-        """Translate English to French:
-
-sea otter => loutre de mer
-
-peppermint => menthe poivrée
-
-plush girafe => girafe peluche
-
-cheese =>""",
+#         # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api
+#         """Tweet: "I hate it when my phone battery dies."
+# Sentiment: Negative
+# ###
+# Tweet: "My day has been 👍"
+# Sentiment: Positive
+# ###
+# Tweet: "This is the link to the article"
+# Sentiment: Neutral
+# ###
+# Tweet: "This new music video was incredibile"
+# Sentiment:""",
+#         """Translate English to French:
+
+# sea otter => loutre de mer
+
+# peppermint => menthe poivrée
+
+# plush girafe => girafe peluche
+
+# cheese =>""",
     ]
+
     results = generator.generate(
         prompts, max_gen_len=256, temperature=temperature, top_p=top_p
     )
+    print("results: ", results)
+
+    #with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU], record_shapes=True) as profiler:
+    #    results = generator.generate(
+    #        prompts, max_gen_len=256, temperature=temperature, top_p=top_p
+    #    )
+
+
+    #pro_table_avg = profiler.key_averages().table(row_limit=-1)
+    #pro_table_events = profiler.events().table(row_limit=-1)
+    #with open("pro_table_avg.txt", "w") as fi:
+    #    fi.write(pro_table_avg)
 
-    for result in results:
-        print(result)
-        print("\n==================================\n")
+    #with open("pro_table_events.txt", "w") as fi:
+    #    fi.write(pro_table_events)
 
 
 if __name__ == "__main__":
diff --git a/llama/generation.py b/llama/generation.py
index 3abd3ed..9a693f3 100755
--- a/llama/generation.py
+++ b/llama/generation.py
@@ -32,7 +32,7 @@ class LLaMA:
 
         total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)
 
-        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()
+        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cpu().long()
         for k, t in enumerate(prompt_tokens):
             tokens[k, : len(t)] = torch.tensor(t).long()
         input_text_mask = tokens != self.tokenizer.pad_id
diff --git a/llama/model.py b/llama/model.py
index baac760..d49eebf 100755
--- a/llama/model.py
+++ b/llama/model.py
@@ -111,10 +111,10 @@ class Attention(nn.Module):
 
         self.cache_k = torch.zeros(
             (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)
-        ).cuda()
+        ).cpu()
         self.cache_v = torch.zeros(
             (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)
-        ).cuda()
+        ).cpu()
 
     def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
         bsz, seqlen, _ = x.shape
